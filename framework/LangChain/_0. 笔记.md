# Model

## 1. 提示模板 PromptTemplate

1. PromptTemplate           常用的 String 提示模板
2. ChatPromptTemplate       常用的 Chat 提示模板
     - ChatMessagePromptTemplate
     - HumanMessagePromptTemplate
     - AIlMessagePromptTemplate
     - SystemMessagePromptTemplate
3. FewShotPromptTemplate    少量样本提示模板
4. 部分格式化提示模板         只传递部分参数，其他的晚点填充
5. PipelinePrompt           管道，用于把几个提示组合在一起使用。
6. 自定义模板                组合其他模板类来定制

## 2. 语言模型

1. 各个 LLM 的封装
   - `ChatOpenAI`、`ChatTongyi`、`HuggingFaceEndpoint`
2. 对聊天模型的抽象封装
   - `SystemMessage`/`HumanMessage`/`AIMessage`
3. 给 embedding 模型提供统一的接口：
   - `DashScopeEmbeddings`、`HuggingFaceEmbeddings`
     - `embeddings.embed_documents(["xxx"])` - 嵌入文档
     - `embeddings.embed_query("你是谁")` - 嵌入查询

## 3. 输出解析

- CSV解析器：`CommaSeparatedListOutputParser`
- JSON解析器：`JsonOutputParser`
- XML解析器：`XMLOutputParser`

---

# RAG

## 1. 文档加载模块 Document loaders

- txt：`TextLoader`
- pdf：`PyPDFLoader`（需要 pypdf 模块，可以传入 url）
- word：`UnstructuredWordDocumentLoader`（需要 unstructured、python-doc 模块）
- excel：`UnstructuredExcelLoader`（需要 unstructured 模块）

```python
loader.load_and_split() # 默认 RecursiveCharacterTextSplitter 分割
loader.load() # 读取所有内容
```

## 2. 文档切分模块 Text Splitter

- **固定字符数** `CharacterTextSplitter`
  - 直接截取，快但是不保证完整
  - 适合简单文本
- **分隔符递归** `RecursiveCharacterTextSplitter`
  - 根据分割符切分，需要手动调优但是保证完整
  - 适合长文本
- **token计数** `TokenTextSplitter`
  - 需要严格适配上下文窗口
  - 适合成本控制
- **嵌入相似度** `SemanticTextSplitter`
  - 基于主题/段落的语义，慢但准确
- **编程语言** `LanguageModelTextSplitter`
  - 源码处理，保留类/函数结构
- **MD文本** `MarkdownTextSplitter`/`MarkdownHeaderTextSplitter`

```python
splitter.create_documents(['1...', '2...']) # [page.page_content for page in pages if pages]
splitter.split_documents(docs) # 底层使用 create_documents

```

## 3. 向量化模型封装

（见 Model Embed 模型）
- 如果使用了向量存储，则自动在里面处理，不需要使用封装

## 4. 向量存储 Vector stores

- 官方提供了三种开源、免费的：chroma、FAISS、Lance
- 内存的 InMemoryVectorStore
- 封装了向量化、检索器等功能

```python
# 自动进行向量化存入
# from_texts(['...', '...'])
db = Chroma.from_documents(paragraphs, DashScopeEmbeddings(...))
# 自动转换为检索器
db.similarity_search("请问14岁犯罪会被释放吗", k=3) # 搜素文档
db.similarity_search_with_score() #带分数
```

## 5. 检索器 Retrievers

- 比 Vector stores 更加通用，输入检索内容，返回文档

```python
# 存储转换为检索器，search_type 默认是 similarity
db.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# 添加
retriever.add_documents([Document(page_content='xxx')])

# 查询
docs = retriever.invoke("会计核算基础规范")
# retriever.get_relevant_documents 已经废弃
```

---

# 链式调用、管道

## 1. 调用

- 管道的实现基于 `Runnable` 接口，使用 `batch` 和 `invoke` 调用

```python
prompt_temp = PromptTemplate.from_template(template)

# 旧的方式，prompt template 使用 .format()
prompt = prompt_temp.format(number=2) # 得到 str
llm.invoke(prompt)

# 旧的链式
chain = LLMChain(llm=llm, prompt=prompt_temp)
chain.invoke({"number": 2})

# 管道
chain = prompt_temp | llm | StrOutputParser()
chain.invoke({"number": 2})
chain.batch([{"number": 2}, {"number": 3}], {"max_concurrency": 5})

# 管道案例
{"rag": retriever, "question": RunnablePassthrough()}
  | prompt
  | llm
  | StrOutputParser() # 解析提取出 response.content

# 管道案例2
template = """请根据下面给出的上下文来回答问题:
            {context}
            问题: {question}
            """
chain = RunnableMap({
    "context": lambda x: retriever.invoke(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | llm | StrOutputParser()

response = chain.invoke({"question": "天才AI少女是谁？"})
```

## 2. 并行和分支

```python
# lambda 封装为 chain
weather_chain = RunnableLambda(
    lambda x: search_tool.invoke(f"周末北京天气")
)
# 并行的 chain，之后会放到 map 中
parallel_chain = RunnableMap({
    "weather": weather_chain,
    "attraction": rag_retriever_chain,
})
# if-else chain，基于传入的参数选择
RunnableBranch(
    (lambda x: x == "True", parallel_chain),
    lambda x: {
      "weather": "未知",
      "attraction": rag_retriever_chain.invoke(x)
    }
)
```

# Agent

> 建议使用 `LangGraph`

## 1. 基本方式

- `create_retriever_tool` - 创建 RAG 工具
- `Tool()` - 创建普通函数工具(LLM 自动判断参数)

基本流程是：
- `prompt`
- `create_openai_functions_agent`
- `AgentExecutor`

```python
tools = [
  # 1. 使用搜索工具
  TavilySearchResults(max_results=1, tavily_api_key=..),
  # 2. 创建RAG工具（基于检索器）
  create_retriever_tool(
      retriever,
      "iPhone_price_search",
      "搜索有关 iPhone 16 的价格信息。对于iPhone 16的任何问题，您必须使用此工具！",
  )，
  # 3. 创建普通函数工具
  Tool(
      name="queryOrderStatus",
      func=lambda id: '已发货' if id % 2 === 0 else '未发货',
      description="根据订单ID查询订单状态",
      args={"id": "订单的ID"}  # LLM 自动判断参数
  ),
]
# 创建agent
agent = create_openai_functions_agent(llm, tools, prompt)
# 创建执行器
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke({"input": "美国2024年总统选举谁胜出了?"})
```

## 2. ReAct

具备反思和自我纠错能力，使用 `initialize_agent` 进行了一些封装，不需要手动调用
- `OPENAI_FUNCTIONS` 适合上面的传统方式
- `ZERO_SHOT_REACT_DESCRIPTION` 零样本推理
- `SELF_ASK_WITH_SEARCH` 自问自答

```python
agent_executor = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS, # 使用OPENAI_FUNCTIONS，它基于OpenAI的Function Calling功能，能更好地处理JSON参数。
    verbose=True,
    handle_parsing_errors=True, # 至关重要！让Agent执行器具备一定的错误处理能力，在解析失败时尝试修复或继续。
    max_iterations=10, # 明确设置最大迭代次数，防止无限循环
    # 对于OPENAI_FUNCTIONS，可以通过agent_kwargs自定义系统消息的一部分
    agent_kwargs={
        "system_message": "你是一个有用的助手。请严格遵守工具调用规范：必须提供完整且有效的JSON参数。"
    }
)
```

## 3. 自我询问 Self-Ask with Search Agent

用搜索回答答案，通过搜索自我询问（实际也是提供问题-搜索-继续问题的样本）

```python
agent = create_self_ask_with_search_agent
```


高级封装函数
> 比较黑盒的封装，用起来比管道简单一点，但是不够灵活


# 记忆

## 1. Chat Messages

管理的简单方式


```python
from langchain_community.chat_message_histories import ChatMessageHistory

history = ChatMessageHistory()
history.add_user_message("hi!")
history.add_user_message("你好")
history.add_ai_message("whats up?")
print(history.messages)
```


## 2. `RunnableWithMessageHistory` 

```python
conversation = RunnableWithMessageHistory(
    base_chain,  # 基础对话链
    get_session_history=get_session_history,  # 从 store 中获取
    input_messages_key="input",  # 输入文本的键名
    history_messages_key="history"  # 历史记录的键名（需与提示模板中的变量名一致）
)
```

## 3. ConversationBufferMemory



# 其他

简化 RAG

`create_stuff_documents_chain`：将检索到的文档内容（{context}）和用户的问题（{input}）按照预定格式填充到 prompt 中，然后发送给 LLM 处理​

`create_retrieval_chain`：将一个检索器（Retriever）和一个文档链（如 create_stuff_documents_chain创建的链）组合起来